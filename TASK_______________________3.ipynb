{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "MfxBBH5f6xBB",
        "outputId": "60010366-c836-48cf-dfa3-de35cdd16d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mindspore\n",
            "  Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (18 kB)\n",
            "Collecting download\n",
            "  Downloading download-0.3.5-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting numpy<2.0.0,>=1.20.0 (from mindspore)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m523.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (4.25.6)\n",
            "Collecting asttokens>=2.0.4 (from mindspore)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (24.2)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.6.3)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (0.5.3)\n",
            "Collecting dill>=0.3.7 (from mindspore)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from download) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from download) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from download) (2.32.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->download) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->download) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->download) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->download) (2025.1.31)\n",
            "Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl (962.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.0/962.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading download-0.3.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, dill, asttokens, download, mindspore\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed asttokens-3.0.0 dill-0.3.9 download-0.3.5 mindspore-2.5.0 numpy-1.26.4\n",
            "Creating data folder...\n",
            "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz (162.2 MB)\n",
            "\n",
            "file_sizes: 100%|████████████████████████████| 170M/170M [00:18<00:00, 9.15MB/s]\n",
            "Extracting tar.gz file...\n",
            "Successfully downloaded / unzipped to ./datasets-cifar10-bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./datasets-cifar10-bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install mindspore download\n",
        "from download import download\n",
        "\n",
        "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/cifar-10-binary.tar.gz\"\n",
        "\n",
        "download(url, \"./datasets-cifar10-bin\", kind=\"tar.gz\", replace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "4Kd7VrRN-qg4",
        "outputId": "c12f0902-7a83-442e-9701-bc7f93ac346d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.38.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5aacccc85215449b9344a0f53b02bf40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mindspore.dataset as ds\n",
        "import mindspore.dataset.vision as vision\n",
        "import os\n",
        "import mindspore.dataset.transforms as transforms\n",
        "from mindspore import dtype as mstype\n",
        "import mindspore as ms\n",
        "\n",
        "data_dir = \"/content/datasets-cifar10-bin/cifar-10-batches-bin\"  # Root directory of the dataset\n",
        "batch_size = 256  # Batch size\n",
        "image_size = 224  # Image size of training data\n",
        "# workers = 4  # Number of parallel workers\n",
        "num_classes = 10  # Number of classes\n",
        "\n",
        "\n",
        "workers = min(8, os.cpu_count())\n",
        "def create_dataset_cifar10(dataset_dir, usage, resize, batch_size, workers):\n",
        "\n",
        "    data_set = ds.Cifar10Dataset(dataset_dir=dataset_dir,\n",
        "                                 usage=usage,\n",
        "                                 num_parallel_workers=workers,\n",
        "                                 shuffle=True)\n",
        "\n",
        "    trans = []\n",
        "    if usage == \"train\":\n",
        "        trans += [\n",
        "            vision.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
        "            vision.RandomHorizontalFlip(prob=0.5)\n",
        "        ]\n",
        "\n",
        "    trans += [\n",
        "        vision.Resize(resize),\n",
        "        vision.Rescale(1.0 / 255.0, 0.0),\n",
        "        vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
        "        vision.HWC2CHW()\n",
        "    ]\n",
        "\n",
        "    target_trans = transforms.TypeCast(mstype.int32)\n",
        "\n",
        "    # Data transformation\n",
        "    data_set = data_set.map(operations=trans,\n",
        "                            input_columns='image',\n",
        "                            num_parallel_workers=workers)\n",
        "\n",
        "    data_set = data_set.map(operations=target_trans,\n",
        "                            input_columns='label',\n",
        "                            num_parallel_workers=workers)\n",
        "\n",
        "    # Batching\n",
        "    data_set = data_set.batch(batch_size)\n",
        "\n",
        "    return data_set\n",
        "\n",
        "\n",
        "# Obtain the preprocessed training and testing datasets\n",
        "\n",
        "dataset_train = create_dataset_cifar10(dataset_dir=data_dir,\n",
        "                                       usage=\"train\",\n",
        "                                       resize=image_size,\n",
        "                                       batch_size=batch_size,\n",
        "                                       workers=workers)\n",
        "step_size_train = dataset_train.get_dataset_size()\n",
        "\n",
        "dataset_val = create_dataset_cifar10(dataset_dir=data_dir,\n",
        "                                     usage=\"test\",\n",
        "                                     resize=image_size,\n",
        "                                     batch_size=batch_size,\n",
        "                                     workers=workers)\n",
        "step_size_val = dataset_val.get_dataset_size()\n"
      ],
      "metadata": {
        "id": "90dyCfy692Jf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Type, Union, List, Optional\n",
        "import mindspore.nn as nn\n",
        "from mindspore.common.initializer import Normal\n",
        "\n",
        "# Initialize the parameters of the convolutional layer and BatchNorm layer\n",
        "weight_init = Normal(mean=0, sigma=0.02)\n",
        "gamma_init = Normal(mean=1, sigma=0.02)\n",
        "\n",
        "class ResidualBlockBase(nn.Cell):\n",
        "    expansion: int = 1  # The number of convolution kernels at the last layer is the same as that of convolution kernels at the first layer.\n",
        "\n",
        "    def __init__(self, in_channel: int, out_channel: int,\n",
        "                 stride: int = 1, norm: Optional[nn.Cell] = None,\n",
        "                 down_sample: Optional[nn.Cell] = None) -> None:\n",
        "        super(ResidualBlockBase, self).__init__()\n",
        "        if not norm:\n",
        "            self.norm = nn.BatchNorm2d(out_channel)\n",
        "        else:\n",
        "            self.norm = norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel,\n",
        "                               kernel_size=3, stride=stride,\n",
        "                               weight_init=weight_init)\n",
        "        self.conv2 = nn.Conv2d(in_channel, out_channel,\n",
        "                               kernel_size=3, weight_init=weight_init)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_sample = down_sample\n",
        "\n",
        "    def construct(self, x):\n",
        "        \"\"\"ResidualBlockBase construct.\"\"\"\n",
        "        identity = x  # shortcut\n",
        "\n",
        "        out = self.conv1(x)  # First layer of the main body: 3 x 3 convolutional layer\n",
        "        out = self.norm(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)  # Second layer of the main body: 3 x 3 convolutional layer\n",
        "        out = self.norm(out)\n",
        "\n",
        "        if self.down_sample is not None:\n",
        "            identity = self.down_sample(x)\n",
        "        out += identity  # output the sum of the main body and the shortcuts\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "MQZ86by9920O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Cell):\n",
        "    expansion = 4  # The number of convolution kernels at the last layer is four times that of convolution kernels at the first layer.\n",
        "\n",
        "    def __init__(self, in_channel: int, out_channel: int,\n",
        "                 stride: int = 1, down_sample: Optional[nn.Cell] = None) -> None:\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel,\n",
        "                               kernel_size=1, weight_init=weight_init)\n",
        "        self.norm1 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel,\n",
        "                               kernel_size=3, stride=stride,\n",
        "                               weight_init=weight_init)\n",
        "        self.norm2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion,\n",
        "                               kernel_size=1, weight_init=weight_init)\n",
        "        self.norm3 = nn.BatchNorm2d(out_channel * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_sample = down_sample\n",
        "\n",
        "    def construct(self, x):\n",
        "\n",
        "        identity = x  # shortcut\n",
        "\n",
        "        out = self.conv1(x)  # First layer of the main body: 1 x 1 convolutional layer\n",
        "        out = self.norm1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)  # Second layer of the main body: 3 x 3 convolutional layer\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)  # Third layer of the main body: 1 x 1 convolutional layer\n",
        "        out = self.norm3(out)\n",
        "\n",
        "        if self.down_sample is not None:\n",
        "            identity = self.down_sample(x)\n",
        "\n",
        "        out += identity  # The output is the sum of the main body and the shortcut.\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-IobdsyT-DGB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_layer(last_out_channel, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "               channel: int, block_nums: int, stride: int = 1):\n",
        "    down_sample = None  # shortcuts\n",
        "\n",
        "\n",
        "    if stride != 1 or last_out_channel != channel * block.expansion:\n",
        "\n",
        "        down_sample = nn.SequentialCell([\n",
        "            nn.Conv2d(last_out_channel, channel * block.expansion,\n",
        "                      kernel_size=1, stride=stride, weight_init=weight_init),\n",
        "            nn.BatchNorm2d(channel * block.expansion, gamma_init=gamma_init)\n",
        "        ])\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(last_out_channel, channel, stride=stride, down_sample=down_sample))\n",
        "\n",
        "    in_channel = channel * block.expansion\n",
        "    # Stack residual networks.\n",
        "    for _ in range(1, block_nums):\n",
        "\n",
        "        layers.append(block(in_channel, channel))\n",
        "\n",
        "    return nn.SequentialCell(layers)\n"
      ],
      "metadata": {
        "id": "O0BrwqDz-Dfv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mindspore import load_checkpoint, load_param_into_net\n",
        "\n",
        "\n",
        "class ResNet(nn.Cell):\n",
        "    def __init__(self, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "                 layer_nums: List[int], num_classes: int, input_channel: int) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        # At the first convolutional layer, the number of the input channels is 3 (color image) and that of the output channels is 64.\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, weight_init=weight_init)\n",
        "        self.norm = nn.BatchNorm2d(64)\n",
        "        # Maximum pooling layer, reducing the image size\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
        "        # Define each residual network structure block\n",
        "        self.layer1 = make_layer(64, block, 64, layer_nums[0])\n",
        "        self.layer2 = make_layer(64 * block.expansion, block, 128, layer_nums[1], stride=2)\n",
        "        self.layer3 = make_layer(128 * block.expansion, block, 256, layer_nums[2], stride=2)\n",
        "        self.layer4 = make_layer(256 * block.expansion, block, 512, layer_nums[3], stride=2)\n",
        "        # average pooling layer\n",
        "        self.avg_pool = nn.AvgPool2d()\n",
        "        # flattern layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        # fully-connected layer\n",
        "        self.fc = nn.Dense(in_channels=input_channel, out_channels=num_classes)\n",
        "\n",
        "    def construct(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JHmRpzdO-F3D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _resnet(model_url: str, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
        "            layers: List[int], num_classes: int, pretrained: bool, pretrained_ckpt: str,\n",
        "            input_channel: int):\n",
        "    model = ResNet(block, layers, num_classes, input_channel)\n",
        "\n",
        "    if pretrained:\n",
        "        # load pre-trained models\n",
        "        download(url=model_url, path=pretrained_ckpt, replace=True)\n",
        "        param_dict = load_checkpoint(pretrained_ckpt)\n",
        "        load_param_into_net(model, param_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(num_classes: int = 1000, pretrained: bool = False):\n",
        "    \"\"\"ResNet50 model\"\"\"\n",
        "    resnet50_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/models/application/resnet50_224_new.ckpt\"\n",
        "    resnet50_ckpt = \"./LoadPretrainedModel/resnet50_224_new.ckpt\"\n",
        "    return _resnet(resnet50_url, ResidualBlock, [3, 4, 6, 3], num_classes,\n",
        "                   pretrained, resnet50_ckpt, 2048)\n"
      ],
      "metadata": {
        "id": "kb8F3cDy-Ibi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ResNet-50 model\n",
        "network = resnet50(pretrained=True)\n",
        "\n",
        "# Modify the fully connected layer to 10 output classes for CIFAR-10\n",
        "in_channel = network.fc.in_channels\n",
        "network.fc = nn.Dense(in_channels=in_channel, out_channels=10)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in network.get_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last 5 layers:\n",
        "# Last 2 blocks of layer4 + fc + last 2 layers of layer3\n",
        "for param in network.layer4[-2:].get_parameters():  # Last 2 blocks of layer4\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in network.fc.get_parameters():  # Fully connected layer\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in network.layer3[-2:].get_parameters():  # Last 2 blocks of layer3\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Print model summary\n",
        "total_params = sum(p.size for p in network.get_parameters())\n",
        "trainable_params = sum(p.size for p in network.get_parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "print(network)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-uOi9ai-Mry",
        "outputId": "6400c67b-a33a-47f1-e98a-f975428d88b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/models/application/resnet50_224_new.ckpt (97.7 MB)\n",
            "\n",
            "file_sizes: 100%|████████████████████████████| 102M/102M [00:10<00:00, 10.0MB/s]\n",
            "Successfully downloaded file to ./LoadPretrainedModel/resnet50_224_new.ckpt\n",
            "Total Parameters: 23581642\n",
            "Trainable Parameters: 11198474\n",
            "ResNet<\n",
            "  (relu): ReLU<>\n",
            "  (conv1): Conv2d<input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "  (norm): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=norm.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=norm.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=norm.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=norm.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "  (max_pool): MaxPool2d<kernel_size=3, stride=2, pad_mode=SAME>\n",
            "  (layer1): SequentialCell<\n",
            "    (0): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.0.norm1.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.0.norm1.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.0.norm1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.norm1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.0.norm2.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.0.norm2.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.0.norm2.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.norm2.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=64, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.0.norm3.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.0.norm3.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.0.norm3.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.norm3.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      (down_sample): SequentialCell<\n",
            "        (0): Conv2d<input_channels=64, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "        (1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.0.down_sample.1.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.0.down_sample.1.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.0.down_sample.1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.down_sample.1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "        >\n",
            "      >\n",
            "    (1): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=256, output_channels=64, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.1.norm1.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.1.norm1.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.1.norm1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.norm1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.1.norm2.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.1.norm2.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.1.norm2.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.norm2.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=64, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.1.norm3.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.1.norm3.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.1.norm3.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.norm3.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (2): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=256, output_channels=64, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.2.norm1.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.2.norm1.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.2.norm1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.2.norm1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.2.norm2.gamma, shape=(64,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.2.norm2.beta, shape=(64,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.2.norm2.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.2.norm2.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=64, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer1.2.norm3.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer1.2.norm3.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer1.2.norm3.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.2.norm3.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    >\n",
            "  (layer2): SequentialCell<\n",
            "    (0): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=256, output_channels=128, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.0.norm1.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.0.norm1.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.0.norm1.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.0.norm1.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=128, output_channels=128, kernel_size=(3, 3), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.0.norm2.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.0.norm2.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.0.norm2.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.0.norm2.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=128, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.0.norm3.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.0.norm3.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.0.norm3.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.0.norm3.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      (down_sample): SequentialCell<\n",
            "        (0): Conv2d<input_channels=256, output_channels=512, kernel_size=(1, 1), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "        (1): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.0.down_sample.1.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.0.down_sample.1.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.0.down_sample.1.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.0.down_sample.1.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "        >\n",
            "      >\n",
            "    (1): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=512, output_channels=128, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.1.norm1.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.1.norm1.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.1.norm1.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.1.norm1.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=128, output_channels=128, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.1.norm2.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.1.norm2.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.1.norm2.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.1.norm2.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=128, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.1.norm3.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.1.norm3.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.1.norm3.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.1.norm3.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (2): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=512, output_channels=128, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.2.norm1.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.2.norm1.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.2.norm1.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.2.norm1.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=128, output_channels=128, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.2.norm2.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.2.norm2.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.2.norm2.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.2.norm2.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=128, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.2.norm3.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.2.norm3.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.2.norm3.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.2.norm3.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (3): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=512, output_channels=128, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.3.norm1.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.3.norm1.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.3.norm1.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.3.norm1.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=128, output_channels=128, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=128, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.3.norm2.gamma, shape=(128,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.3.norm2.beta, shape=(128,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.3.norm2.moving_mean, shape=(128,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.3.norm2.moving_variance, shape=(128,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=128, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer2.3.norm3.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer2.3.norm3.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer2.3.norm3.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer2.3.norm3.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    >\n",
            "  (layer3): SequentialCell<\n",
            "    (0): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=512, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.0.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.0.norm1.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.0.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.0.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.0.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.0.norm2.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.0.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.0.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.0.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.0.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.0.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.0.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      (down_sample): SequentialCell<\n",
            "        (0): Conv2d<input_channels=512, output_channels=1024, kernel_size=(1, 1), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "        (1): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.0.down_sample.1.gamma, shape=(1024,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.0.down_sample.1.beta, shape=(1024,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.0.down_sample.1.moving_mean, shape=(1024,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.0.down_sample.1.moving_variance, shape=(1024,), dtype=Float32, requires_grad=False)>\n",
            "        >\n",
            "      >\n",
            "    (1): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.1.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.1.norm1.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.1.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.1.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.1.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.1.norm2.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.1.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.1.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.1.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.1.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.1.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.1.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (2): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.2.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.2.norm1.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.2.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.2.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.2.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.2.norm2.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.2.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.2.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.2.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.2.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.2.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.2.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (3): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.3.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.3.norm1.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.3.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.3.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.3.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.3.norm2.beta, shape=(256,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.3.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.3.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer3.3.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer3.3.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer3.3.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer3.3.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (4): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=4.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=4.norm1.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=4.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=4.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=True)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=4.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=4.norm2.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=4.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=4.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=True)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=4.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=True), beta=Parameter (name=4.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=4.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=4.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=True)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (5): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=5.norm1.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=5.norm1.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=5.norm1.moving_mean, shape=(256,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=5.norm1.moving_variance, shape=(256,), dtype=Float32, requires_grad=True)>\n",
            "      (conv2): Conv2d<input_channels=256, output_channels=256, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=256, eps=1e-05, momentum=0.9, gamma=Parameter (name=5.norm2.gamma, shape=(256,), dtype=Float32, requires_grad=True), beta=Parameter (name=5.norm2.beta, shape=(256,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=5.norm2.moving_mean, shape=(256,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=5.norm2.moving_variance, shape=(256,), dtype=Float32, requires_grad=True)>\n",
            "      (conv3): Conv2d<input_channels=256, output_channels=1024, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=1024, eps=1e-05, momentum=0.9, gamma=Parameter (name=5.norm3.gamma, shape=(1024,), dtype=Float32, requires_grad=True), beta=Parameter (name=5.norm3.beta, shape=(1024,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=5.norm3.moving_mean, shape=(1024,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=5.norm3.moving_variance, shape=(1024,), dtype=Float32, requires_grad=True)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    >\n",
            "  (layer4): SequentialCell<\n",
            "    (0): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=1024, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer4.0.norm1.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer4.0.norm1.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer4.0.norm1.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer4.0.norm1.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (conv2): Conv2d<input_channels=512, output_channels=512, kernel_size=(3, 3), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer4.0.norm2.gamma, shape=(512,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer4.0.norm2.beta, shape=(512,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer4.0.norm2.moving_mean, shape=(512,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer4.0.norm2.moving_variance, shape=(512,), dtype=Float32, requires_grad=False)>\n",
            "      (conv3): Conv2d<input_channels=512, output_channels=2048, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=2048, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer4.0.norm3.gamma, shape=(2048,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer4.0.norm3.beta, shape=(2048,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer4.0.norm3.moving_mean, shape=(2048,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer4.0.norm3.moving_variance, shape=(2048,), dtype=Float32, requires_grad=False)>\n",
            "      (relu): ReLU<>\n",
            "      (down_sample): SequentialCell<\n",
            "        (0): Conv2d<input_channels=1024, output_channels=2048, kernel_size=(1, 1), stride=(2, 2), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "        (1): BatchNorm2d<num_features=2048, eps=1e-05, momentum=0.9, gamma=Parameter (name=layer4.0.down_sample.1.gamma, shape=(2048,), dtype=Float32, requires_grad=False), beta=Parameter (name=layer4.0.down_sample.1.beta, shape=(2048,), dtype=Float32, requires_grad=False), moving_mean=Parameter (name=layer4.0.down_sample.1.moving_mean, shape=(2048,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer4.0.down_sample.1.moving_variance, shape=(2048,), dtype=Float32, requires_grad=False)>\n",
            "        >\n",
            "      >\n",
            "    (1): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=2048, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=1.norm1.gamma, shape=(512,), dtype=Float32, requires_grad=True), beta=Parameter (name=1.norm1.beta, shape=(512,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=1.norm1.moving_mean, shape=(512,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=1.norm1.moving_variance, shape=(512,), dtype=Float32, requires_grad=True)>\n",
            "      (conv2): Conv2d<input_channels=512, output_channels=512, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=1.norm2.gamma, shape=(512,), dtype=Float32, requires_grad=True), beta=Parameter (name=1.norm2.beta, shape=(512,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=1.norm2.moving_mean, shape=(512,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=1.norm2.moving_variance, shape=(512,), dtype=Float32, requires_grad=True)>\n",
            "      (conv3): Conv2d<input_channels=512, output_channels=2048, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=2048, eps=1e-05, momentum=0.9, gamma=Parameter (name=1.norm3.gamma, shape=(2048,), dtype=Float32, requires_grad=True), beta=Parameter (name=1.norm3.beta, shape=(2048,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=1.norm3.moving_mean, shape=(2048,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=1.norm3.moving_variance, shape=(2048,), dtype=Float32, requires_grad=True)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    (2): ResidualBlock<\n",
            "      (conv1): Conv2d<input_channels=2048, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm1): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=2.norm1.gamma, shape=(512,), dtype=Float32, requires_grad=True), beta=Parameter (name=2.norm1.beta, shape=(512,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=2.norm1.moving_mean, shape=(512,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=2.norm1.moving_variance, shape=(512,), dtype=Float32, requires_grad=True)>\n",
            "      (conv2): Conv2d<input_channels=512, output_channels=512, kernel_size=(3, 3), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm2): BatchNorm2d<num_features=512, eps=1e-05, momentum=0.9, gamma=Parameter (name=2.norm2.gamma, shape=(512,), dtype=Float32, requires_grad=True), beta=Parameter (name=2.norm2.beta, shape=(512,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=2.norm2.moving_mean, shape=(512,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=2.norm2.moving_variance, shape=(512,), dtype=Float32, requires_grad=True)>\n",
            "      (conv3): Conv2d<input_channels=512, output_channels=2048, kernel_size=(1, 1), stride=(1, 1), pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=<mindspore.common.initializer.Normal object at 0x784bcd5c7050>, bias_init=None, format=NCHW>\n",
            "      (norm3): BatchNorm2d<num_features=2048, eps=1e-05, momentum=0.9, gamma=Parameter (name=2.norm3.gamma, shape=(2048,), dtype=Float32, requires_grad=True), beta=Parameter (name=2.norm3.beta, shape=(2048,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=2.norm3.moving_mean, shape=(2048,), dtype=Float32, requires_grad=True), moving_variance=Parameter (name=2.norm3.moving_variance, shape=(2048,), dtype=Float32, requires_grad=True)>\n",
            "      (relu): ReLU<>\n",
            "      >\n",
            "    >\n",
            "  (avg_pool): AvgPool2d<kernel_size=1, stride=1, pad_mode=VALID>\n",
            "  (flatten): Flatten<>\n",
            "  (fc): Dense<input_channels=2048, output_channels=10, has_bias=True>\n",
            "  >\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5XXWI9J-6YH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}